import json
import re
import pandas as pd
from tqdm import tqdm
from utils import (
    sample_from_two_lists,
    get_dfs_info,
    get_tool,
    filter_code,
    read_jsonl,
    filter_cot,
    timeout,
    TimeoutException,
    execute_with_timeout,
    load_json,
    save_json
)
from table_qa_execution_eval.sft_prompt import (
    prompt_with_format_list,
    prompt_with_instruction_list,
)
from inference import (
    generate_outputs,
    load_model,
    load_tokenizer_and_template,
    get_infer_kwargs,
)
import os
import argparse
import shutil
from pathlib import Path

CODE_PREFIX = """import matplotlib.pyplot as plt
from mplfonts import use_font
import pandas as pd
import numpy as np
import seaborn as sns
import warnings

warnings.filterwarnings("ignore")
# Fixing Chinese font issues
use_font("Noto Serif CJK SC")
plt.rcParams['font.sans-serif']=['SimHei']
plt.rcParams['axes.unicode_minus']=False\n"""


def format_inputs(test_datas: list[dict]) -> list[list[dict]]:
    """Format inputs to the required messages"""
    # 把需要推理的数据拼成 message 形式
    format_message_datas = []
    for idx, test_dt in enumerate(test_datas):
        # eval_instruction = sample_from_two_lists(
        #     prompt_with_format_list, prompt_with_instruction_list
        # )
        # query = test_dt["query"]
        # table_paths = test_dt["table_paths"]
        # dfs_infos = get_dfs_info(table_paths)
        # format_instruction = eval_instruction.format(df_info=dfs_infos, input=query)

        messages = [{"role": "user", "content": test_dt["instruction"]}]
        format_message_datas.append(messages)

    return format_message_datas


def eval_outputs(
    model_outputs: list[dict],
    eval_dataset_path: str,
) -> list[dict]:
    test_datas = load_json(eval_dataset_path)

    output_texts = [i["output_text"] for i in model_outputs]
    processed_data = []
    for idx, test_dt in enumerate(test_datas):
        llm_output = output_texts[idx]
        df_paths = test_datas[idx]["table_paths"]
        df_names = test_datas[idx]["df_names"]
        query = test_datas[idx]["query"]
        instruction = test_datas[idx]["instruction"]
        table_paths = test_datas[idx]["table_paths"]
        # df_infos = get_dfs_info(df_paths)
        eval_result_sample = {}
        df = [pd.read_csv(path, low_memory=False) for path in df_paths]
        # if len(df_paths) == 1:
        #     df = pd.read_csv(df_paths[0], low_memory=False)
        # else:
        #     df = [pd.read_csv(path, low_memory=False) for path in df_paths]
        tool = get_tool(df,df_names)
        code, _ = filter_code(llm_output)
        cot = filter_cot(llm_output)

        # 运行超时代码，认为都是异常代码， 在tool.run()过程中，可能会print出额外的内容，不影响执行
        try:
            # 如果生成的代码为空（解析不到代码）， 也认为是llm没有理解observe内容或instruct， 输出为Code Error
            if not code:
                observe = "Code Error: output empty code.."
            else:
                with timeout(15):  # 设置超时时间为15秒
                    # pure_code = CODE_PREFIX + pure_code
                    pure_code = CODE_PREFIX + code
                    # print("pure code:", pure_code)
                    observe = tool.run(pure_code)  # 需要监控超时的代码块
                    # observe = execute_with_timeout(pure_code, 15, tool)
                    if isinstance(observe, pd.DataFrame):
                        observe = observe.head().to_markdown(index=False)
                    else:
                        observe = str(observe)
        except TimeoutException as e:
            observe = f"Timeout Error: code running time exceed 15s.."
        except SystemExit as e:
            observe = f"SystemExit Error: {str(e)}"
        except Exception as e:
            observe = f"Unexpected Error: {str(e)}"

        eval_result_sample["code"] = CODE_PREFIX + code
        # eval_result_sample["pure_code"] = CODE_PREFIX + pure_code
        eval_result_sample["llm_output"] = llm_output
        eval_result_sample["cot"] = cot
        eval_result_sample["observe"] = observe
        eval_result_sample["input"] = instruction
        eval_result_sample["query"] = query
        eval_result_sample["table_paths"] = table_paths

        pattern = re.compile(r"error|exception", re.IGNORECASE)
        try:
            flag = not pattern.search(observe)
        except:
            flag = True
        eval_result_sample["flag"] = flag
        processed_data.append(eval_result_sample)
    return processed_data


def execution_eval(observe: str) -> bool:
    """
    Test whether the code generated by eval_llm can be executed.
    :param output: output code of llm generation
    :return: True or False
    """
    # 只要执行结果中不出现error 或者 exception， 就认为代码可执行
    pattern = re.compile(r"error|exception", re.IGNORECASE)

    try:
        res = not pattern.search(observe)
    except:
        res = True
    # print("Execute result:", observe)
    # print("Execute passed:", res)
    return res


def run_eval(
    eval_result_path: str = "evalset/test_results/eval_results.json",
):
    with open(eval_result_path, "r", encoding="utf-8") as f:
        samples = json.load(f)
    execute_passed = 0
    total_len = len(samples)
    for sample in tqdm(samples):
        observe = sample["observe"]
        execute_passed += 1 if execution_eval(observe) else 0
    print(f"Sample length: {total_len}. ")

    print(
        f"Execute Passed: {execute_passed}." f"\tExecute pass-rate is:",
        round(execute_passed / total_len, 3),
    )


def main(args):
    eval_dataset_path = args.eval_dataset_path
    eval_results_save_path = args.eval_results_save_path
    model_path = args.model_path
    max_model_len = args.max_model_len
    template = args.template
    gpus_num = args.gpus_num
    model_kwargs = get_infer_kwargs(args)
    print("Load model...")
    llm_model = load_model(model_path, max_model_len, gpus_num)
    tokenizer = load_tokenizer_and_template(model_path, template)
    eval_dataset_path = args.eval_dataset_path
    # test_datas = read_jsonl(eval_dataset_path)
    test_datas = load_json(eval_dataset_path)

    format_message_datas = format_inputs(test_datas)

    print("Generating eval answers now..")
    model_outputs = generate_outputs(
        format_message_datas, llm_model, tokenizer, model_kwargs
    )
    print("Generating answers finished..")

    eval_answers = eval_outputs(model_outputs, eval_dataset_path)

    print("Eval answers construct complete..")
    with open(eval_results_save_path, "w", encoding="utf-8") as f:
        json.dump(eval_answers, f, ensure_ascii=False)

    run_eval(eval_result_path=eval_results_save_path)


if __name__ == "__main__":
    # 确定images目录是否存在和写权限
    output_dir = Path(__file__).parent / "images"
    if os.path.exists(output_dir):
        if not os.access(output_dir, os.W_OK):
            shutil.rmtree(output_dir)
            os.makedirs(output_dir)
            os.chmod(output_dir, 0o777)
            print("not write permission, makedir:", output_dir)
        else:
            print(f"{output_dir} exists!")
    else:
        os.makedirs(output_dir)
        os.chmod(output_dir, 0o777)
        print("makedir:", output_dir)
    parser = argparse.ArgumentParser(description="eval tableqa python code")
    parser.add_argument(
        "--gpus_num", type=int, default=1, help="the number of GPUs you want to use."
    )
    parser.add_argument(
        "--temperature", type=float, default=0.01, help="Temperature setting"
    )

    parser.add_argument(
        "--template",
        type=str,
        choices=[None, "llama3", "baichuan", "chatglm"],
        default=None,
        help="The template must be specified if not present in the config file",
    )

    parser.add_argument(
        "--model_path", type=str, required=True, help="Path to the model"
    )
    parser.add_argument(
        "--model_type",
        choices=["base_model", "chat_model"],
        default="chat_model",
        help="Base model or Chat model",
    )

    parser.add_argument(
        "--max_new_tokens",
        type=int,
        default=1024,
        help="Maximum number of output tokens",
    )
    parser.add_argument("--max_model_len", type=int, default=8192, help="Cutoff length")
    parser.add_argument(
        "--eval_dataset_path",
        type=str,
        default="evalset/table_qa_execuate_test/test_datas_zuizong.json",
        help="Test Set Path",
    )

    parser.add_argument(
        "--eval_results_save_path",
        type=str,
        default="output/result_table_qa.json",
        help="Max iteration for llm to run each code correction task",
    )
    args = parser.parse_args()
    main(args)
    """
    python run_eval.py --model_path /data0/pretrained-models/Qwen2-7B-Instruct
    """
