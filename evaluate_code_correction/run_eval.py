# -*- coding: utf-8 -*-
"""
@Time ： 2024/5/25 15:39
@Auth ： zhaliangyu
@File ：run_eval.py
@IDE ：PyCharm
"""
from tqdm import tqdm
import datetime
from typing import Literal, Optional, Any

import pandas as pd
from langchain_core.language_models import BaseLanguageModel
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.base import Chain
from langchain_experimental.tools.python.tool import PythonAstREPLTool
from langchain_core.output_parsers import StrOutputParser
from evaluate_code_correction.prompt import CLASSIFY_PROMPT_PYTHON, \
    RECTIFY_PROMPT_PYTHON
from evaluate_code_correction.utils import get_tool, create_agent
from evaluate_code_correction.llms import llm_judge
from util import start_service, is_service_up

CODE_PREFIX = """import matplotlib.pyplot as plt
from mplfonts import use_font
import pandas as pd
import numpy as np
import seaborn as sns
import warnings

warnings.filterwarnings("ignore")
# Fixing Chinese font issues
use_font("Noto Serif CJK SC")"""

def main(args):
    """main function to run the code correction evaluation"""
    model_path = args.model_path
    model_name = args.model_name
    service_process = start_service(model_path, model_name, max_len, port)


def pass_rate(sample_len: int, passed: int) -> float:
    """
    :param sample_len: Testing sample length
    :param passed: The number of samples passed test
    :return: pass_rate
    """
    return passed / sample_len


def execution_eval(output_code: str, dfs: Any) -> bool:
    """
    :param output: output code of llm generation
    :return: True or False
    """
    import re
    python_executor = get_tool(dfs)
    code = output_code.strip(" ").strip('"')
    code_res = python_executor.run(code)
    # 只要执行结果中不出现error 或者 exception， 就认为代码可执行
    pattern = re.compile(r"error|exception", re.IGNORECASE)

    try:
        res = not pattern.search(code_res)
    except:
        res = True
    print("Execute Observe:", code_res)
    print("Execute Result:", res)
    return res


def llm_eval(
        query: str,
        table_infos,
        code: str,
        observation: str,
        true_result,
        llm: BaseLanguageModel,
) -> bool:
    """
    :param query: Human input query
    :param table_infos: Input table information
    :param code: Code generated by the llm_eval
    :param observation: Code execution result
    :param llm: llm_judge
    :return: Enum [True, False]
    """
    print("True result", true_result)
    prompt = ChatPromptTemplate.from_messages(
        [("system", CLASSIFY_PROMPT_PYTHON)]
    )
    eval_chain = prompt | llm | StrOutputParser()
    # eval_chain.verbose = True
    input = {
        "query": query,
        # "table_infos": table_infos,
        "code": code,
        "observation": observation,
        "true_result": true_result
    }
    output = eval_chain.invoke(
        input=input
    )
    res = output
    print("Observe:", observation)
    print("LLM eval results: ", res)
    return True if res.lower() == "yes" else False


def eval_pass_k(last_output_k: str, result: str):
    pass


def gen_answer(
        queries: str,
        table_infos: str,
        outputs: str,
        observations: str,
        answer_chain: Chain,
):
    time_now = datetime.datetime.now().strftime('%Y-%m-%d:%H')
    results = answer_chain.invoke(
        input={"query": queries, "table_infos": table_infos,
               "observe": observations, "output": outputs,
               "current_time": time_now
               })
    return results


def get_results(eval_dataset_path: str,
                lan_type: str = "python",
                k: int = 1,
                result_path: str = "../evalset/code_correction_test/results.json"):
    """Generate all llm-eval results"""
    import json
    with open(eval_dataset_path, "r") as f:
        test_samples = json.load(f)
    # sample_len = len(test_samples)
    # # mini_batches = split_batch(test_samples, batch_size)
    passed = 0
    if lan_type == "python":
        prompt_answer_gen = ChatPromptTemplate.from_messages(
            [("system", RECTIFY_PROMPT_PYTHON)]
        )
    else:
        raise Exception(f"Do not support {lan_type} sample evaluation now..")

    answers = []
    for sample in tqdm(test_samples):
        eval_answer_sample = {}
        df_paths = sample["table_paths"]
        if len(df_paths) == 1:
            df = pd.read_csv(df_paths[0])
        else:
            df = [pd.read_csv("../"+path) for path in df_paths]
        tool = get_tool(df)
        tools = [tool]
        agent = create_agent(prompt_answer_gen, llm_for_eval, tools, k)
        true_result = sample["true_result"]
        queries = sample["query"]
        table_infos = sample["table_infos"]
        outputs = sample["cot"] + f"{lan_type} Code:\n" + sample["code"]
        observes = sample["observation"]
        eval_answer = gen_answer(queries, table_infos, outputs, observes, agent)
        code = eval_answer["intermediate_steps"][-1][0].tool_input
        eval_answer_sample["query"] = queries
        eval_answer_sample["observe"] = eval_answer["intermediate_steps"][-1][1]
        eval_answer_sample["code"] = code
        eval_answer_sample["true_result"] = true_result
        eval_answer_sample["table_infos"] = table_infos
        eval_answer_sample["table_paths"] = df_paths
        answers.append(eval_answer_sample)
    with open(result_path, "w") as f:
        json.dump(answers, f,ensure_ascii=False)


def run_eval(
    eval_result_path: str = "../evalset/code_correction_test/results.json",
    llm_for_judge: Optional[BaseLanguageModel] = llm_judge
):
    """
    :param eval_results_path:  Evaluation dataset path
    :param llm_for_judge: llm for classify the content generated by the llm_eval, this param is used while `eval_method == "execution",`
    :return: pass rate
    """
        # print(eval_answer)
    import json
    with open(eval_result_path, "r") as f:
        samples = json.load(f)
    execute_passed, llm_eval_passed = 0, 0
    total_len = len(samples)
    for sample in tqdm(samples):
        code = sample["code"]
        observe = sample["observe"]
        true_result = sample["true_result"]
        table_infos = sample["table_infos"]
        df_paths = sample["table_paths"]
        query = sample["query"]
        if len(df_paths) == 1:
            df = pd.read_csv(df_paths[0])
        else:
            df = [pd.read_csv(path) for path in df_paths]
        execute_passed += 1 if execution_eval(code, df) else 0
        llm_eval_passed += 1 if llm_eval(query,
                                table_infos,
                                code, observe,
                                true_result, llm_for_judge) else 0
        print("*" * 20)
    print(f"Sample length: {total_len}. "
          f"Execute Passed: {execute_passed}."
          f"LLM eval Passed: {llm_eval_passed}")
    print(f"Execute pass-rate is:", round(execute_passed / total_len, 3))
    print(f"LLM_eval pass-rate is:", round(llm_eval_passed / total_len, 3))
    result = {
        "Execute pass-rate": round(execute_passed / total_len, 3),
        "LLM_eval pass-rate": round(llm_eval_passed / total_len, 3)
    }
    with open("../evalset/code_correction_test/Eval_result.json", "w") as f:
        json.dump(result, f, ensure_ascii=False)

if __name__ == "__main__":
    import argparse

    get_results(eval_dataset_path="../evalset/code_correction_test/correction_set_new.json")
    run_eval(eval_result_path="../evalset/code_correction_test/results.json")

