# -*- coding: utf-8 -*-
"""
@Time ： 2024/5/25 15:39
@Auth ： zhaliangyu
@File ：run_eval.py
@IDE ：PyCharm
"""
from tqdm import tqdm
import datetime
from typing import Literal, Optional, Any

import pandas as pd
from langchain_core.language_models import BaseLanguageModel
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.base import Chain
from langchain_experimental.tools.python.tool import PythonAstREPLTool
from langchain_core.output_parsers import StrOutputParser
from evaluate_code_correction.prompt import CLASSIFY_PROMPT_PYTHON, RECTIFY_PROMPT_PYTHON
from evaluate_code_correction.utils import get_tool, create_agent
from llms import llm_for_eval, llm_judge

CODE_PREFIX = """import matplotlib.pyplot as plt
from mplfonts import use_font
import pandas as pd
import numpy as np
import seaborn as sns
import warnings

warnings.filterwarnings("ignore")
# Fixing Chinese font issues
use_font("Noto Serif CJK SC")"""

def pass_rate(sample_len: int, passed: int) -> float:
    """
    :param sample_len: Testing sample length
    :param passed: The number of samples passed test
    :return: pass_rate
    """
    return passed / sample_len

def execution_eval(output_code: str, dfs: Any) -> bool:
    """
    :param output: output code of llm generation
    :return: True or False
    """
    import re
    python_executor = get_tool(dfs)
    code = output_code.strip(" ").strip('"')
    code_res = python_executor.run(code)
    # 只要执行结果中不出现error 或者 exception， 就认为代码可执行
    pattern = re.compile(r"error|exception", re.IGNORECASE)

    try:
        res = not pattern.search(code_res)
    except:
        res = True
    print("Observe:", code_res, "Result:", res)
    return res

def llm_eval(
    query: str,
    table_infos,
    code: str,
    observation: str,
    true_result,
    llm: BaseLanguageModel,
) -> bool:
    """
    :param query: Human input query
    :param table_infos: Input table information
    :param code: Code generated by the llm_eval
    :param observation: Code execution result
    :param llm: llm_judge
    :return: Enum [True, False]
    """
    prompt = ChatPromptTemplate.from_messages(
        [("system", CLASSIFY_PROMPT_PYTHON)]
    )
    eval_chain = llm | prompt| StrOutputParser()
    input = {
        "query": query,
        "table_infos": table_infos,
        "code": code,
        "observation": observation,
        "true_result": true_result
    }
    output = eval_chain.invoke(
        input = input
    )
    res = output[eval_chain.output_key]
    return True if res.lower() == "yes" else False

def eval_pass_k(last_output_k: str, result: str):
    pass


def gen_answer(
    queries: str,
    table_infos: str,
    outputs: str,
    observations: str,
    answer_chain: Chain,
):
    time_now = datetime.datetime.now().strftime('%Y-%m-%d:%H')
    results = answer_chain.invoke(
        input = {"query": queries, "table_infos": table_infos,
                   "observe": observations, "output": outputs, "current_time":time_now
                   })
    return results

def run_eval(
    eval_dataset_path: str,
    eval_method: Literal["execution", "pass_k", "llm"],
    llm_for_eval: BaseLanguageModel,
    llm_for_judge: Optional[BaseLanguageModel] = llm_judge,
    k: Optional[int] = None,
    lan_type: str = "Python"
) :
    """
    :param eval_dataset_path:  Evaluation dataset path
    :param eval_method: Enum ["execution", "pass_k_acc", "llm"]
    :param llm_for_eval: llm for evaluation generation
    :param llm_for_judge: llm for classify the content generated by the llm_eval, this param is used while `eval_method == "execution",`
    :param k: llm_eval_iteration
    :param batch_size: Evaluation answer gen batch size batch size
    :param lan_type: Evaluation language type, only support `python` now
    :return: pass rate
    """
    import json
    with open(eval_dataset_path, "r" ) as f:
        test_samples = json.load(f)
    sample_len = len(test_samples)
    # mini_batches = split_batch(test_samples, batch_size)
    passed = 0
    if lan_type == "Python":
        prompt_answer_gen = ChatPromptTemplate.from_messages(
            [("system", RECTIFY_PROMPT_PYTHON)]
        )
    else:
        raise Exception(f"Do not support {lan_type} sample evaluation now..")

    for sample in tqdm(test_samples[2:]):
        df_paths = sample["table_paths"]
        if len(df_paths) == 1:
            df = pd.read_csv(df_paths[0])
        else:
            df = [pd.read_csv(path) for path in df_paths]
        tool = get_tool(df)
        tools = [tool]
        agent = create_agent(prompt_answer_gen, llm_for_eval, tools, k)
        queries = sample["query"]
        table_infos = sample["table_infos"]
        outputs = sample["cot"] + f"{lan_type} Code:\n" + sample["code"]
        observes = sample["observation"]
        eval_answer = gen_answer(queries, table_infos,outputs, observes, agent)
        # print(eval_answer)

        if eval_method == "execution":
            code = eval_answer["intermediate_steps"][-1][0].tool_input
            passed += 1 if execution_eval(code, df) else 0
            print(passed)
        elif eval_method == "llm":
            true_result = sample["true_result"]
            passed += 1 if llm_eval(queries,
                                    table_infos,
                                    outputs, observes,
                                    true_result, llm_for_judge) else passed
        elif eval_method == "pass_k_acc":
            true_result = sample["true_result"]
            passed += 1 if eval_pass_k(
                outputs, true_result
            ) else passed
        else:
            raise Exception("Eval method do not support")
    pass_r = pass_rate(sample_len, passed)
    print(f"Sample length: {sample_len}. Passed number: {passed}")
    print(f"{eval_method} pass_{k} rate is:", pass_r)


if __name__ == "__main__":
    run_eval(
    eval_dataset_path ="./test_data/test_samples.json",
    llm_for_eval=llm_for_eval,
    llm_for_judge=None,
    eval_method="execution",
    k=1)